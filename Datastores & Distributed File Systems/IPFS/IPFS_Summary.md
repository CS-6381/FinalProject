|  | IPFS (InterPlanetary File System) |
| ------- | ---- |
|**Website(s)**| {1} location for all IPFS documentation [https://docs.ipfs.io](https://docs.ipfs.io) {2} GitHub repository for IPSF Desktop [https://github.com/ipfs/ipfs-desktop](https://github.com/ipfs/ipfs-desktop) 
|**Overview Basics**    | Peer-to-Peer Hypermedia Protocol
|**Implementation Basics**     | {1} Command-Line Quick Start Reference [https://docs.ipfs.io/how-to/command-line-quick-start/#take-your-node-online](https://docs.ipfs.io/how-to/command-line-quick-start/#take-your-node-online) {2} CLI API [https://docs.ipfs.io/reference/cli/#ipfs](https://docs.ipfs.io/reference/cli/#ipfs) 
|**Use Cases**      | {1} As a mounted global filesystem, under /ipfs and /ipns. {2} As a mounted personal sync folder that automatically versions, publishes, and backs up any writes. {3} As an encrypted file or data sharing system. {4} As a versioned package manager for all software. {5} As the root filesystem of a Virtual Machine. {6} As the boot filesystem of a VM (under a hypervisor). {7} As a database: applications can write directly to the Merkle DAG data model and get all the versioning, caching, and distribution IPFS provides. {8} As a linked (and encrypted) communications platform. {9} As an integrity checked CDN for large files (without SSL). {10} As an encrypted CDN. {11} On webpages, as a web CDN. {12} As a new Permanent Web where links do not die.
|**Features**      | {1} Decentralized sharing of files (P2P). {2} Cryptographic hash node IDs (both a public and private ID), {3} Can use any transport protocol. {4} Use of DHTs (based on S/Kademlia and Coral) {5} Data distribution through block exchange using their novel BitSwap protocol (inspirired by BitTorrent) {6} Utilzies a Merkle DAG to link objects that have unique cryptographic hashes that allow for content addressing, tamper resistance, and deduplication. {7} a set of objects for modeling a versioned filesystem on top of the Merkle DAG very similar to Github. {8} Mutable naming.
|**Scalability**     |Highly Scalable
|**Goals**   | (1)Hosting/Distributing large (Petabyte) datasets. (2)Computing on large data across companies (3)High Volume or high definition on-demand or realtime media streams. (4)versioning and linking  of massive datasets. (5)Low latency and decentralized file system distribution. 

